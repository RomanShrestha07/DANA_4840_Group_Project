---
title: "DANA4840 Project - Hierarchical Clustering"
author: "M.Gadimova, A.Mukherjee, R.Shrestha, P.Tating"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

# 1. Research Statement on Cars Dataset

The automobile industry continually seeks to understand the characteristics and performance of various car models in order to meet the varied needs of its customers. The given dataset contains numerous characteristics that can be used to categorize cars into different groups, including weight (wt), horsepower (hp), engine displacement (disp), and miles per gallon (mpg). 

The objective of this analysis is to segment the car models into clusters that represent different types of vehicles. This can help in understanding the underlying patterns in the data, such as identifying clusters of high-performance sports cars, fuel-efficient vehicles, or family-oriented models.Additionally, by using this clustering, marketers and producers can more successfully target particular customer categories.


# 2. Preliminaries

Before diving into the cluster analysis, let's first thoroughly examine and understand our data. This preliminary step will allow us to identify key patterns and characteristics within the dataset, ensuring a solid foundation for accurate analysis. By doing so, we can address any potential data quality issues and refine our approach for more meaningful results.

## 2.1 Loading the Libraries

```{r}
library("tidyverse")
library("factoextra")
library("dendextend")
library("hopkins")
library("corrplot")
library("cluster")
library("patchwork")
library("clValid")

set.seed(101)
```


## 2.2 Reading the Data

```{r}
mtcars <- read.csv("data/mtcars.csv", header = T, sep = ",")
head(mtcars)
```


## 2.3 Feature Explanation

The 'mtcars' dataset includes 12 features as detailed below:

* 'model' (categorical): Model of the vehicle

* 'mpg' (numerical): Miles/(US) gallon, a measure of fuel efficiency

* 'cyl' (categorical-ordinal): Number of cylinders in the engine

* 'disp' (numerical): Displacement in cubic inches

* 'hp' (numerical): Gross horsepower

* 'drat' (numerical): Rear axle ratio

* 'wt' (numerical): Weight of the car (1000 lbs)

* 'qsec' (numerical): 1/4 mile time, a measure of acceleration

* 'vs' (binary): Engine type (0 = V-shaped, 1 = straight)

* 'am' (binary): Transmission type (0 = automatic, 1 = manual)

* 'gear' (categorical-ordinal): Number of forward gears

* 'carb' (categorical-ordinal): Number of carburetor


## 2.4 Exploratory Data Analysis

```{r}
str(mtcars)
```

```{r}
rownames(mtcars)
```


## 2.5 Data Pre-processing

```{r}
mtcars_categorical <- data.frame(
  cyl = mtcars$cyl,
  vs = mtcars$vs,
  am = mtcars$am,
  gear = mtcars$gear,
  carb = mtcars$carb
)

mtcars_numerical <- data.frame(
  mpg = mtcars$mpg,
  disp = mtcars$disp,
  hp = mtcars$hp,
  drat = mtcars$drat,
  wt = mtcars$wt,
  qsec = mtcars$qsec
)

mtcars_numerical_scaled <- data.frame(scale(mtcars_numerical))

mtcars_joined <- cbind(mtcars_numerical_scaled, mtcars_categorical)
rownames(mtcars_joined) <- mtcars$model
mtcars <- mtcars_joined
head(mtcars)
```


# 3. Pre-clustering Assessment

Before performing clustering analysis, it is crucial to conduct a pre-clustering assessment to evaluate the dataset's cluster tendency and determine the optimal clustering approach. Tools like the Hopkins statistic and VAT can help assess whether the data points possess significant clustering tendencies. Once cluster tendency is established, the next step involves finding the optimal number of clusters. This can be achieved using methods such as the Elbow Method, Silhouette Analysis, or the Gap Statistic, each providing insights into the most meaningful way to partition the data. 

## 3.1 Assessing Cluster Tendency

```{r}
fviz_pca_ind(
  prcomp(mtcars),
  title = "PCA - mtcars",
  palette = "jco",
  geom = "point",
  ggtheme = theme_classic(),
  legend = "bottom"
)
```

### 3.1.1 Hopkins Statistics

```{r}
set.seed(25)

hopkins_mtcars <- hopkins(mtcars, m = ceiling(nrow(mtcars) / 10))
hopkins_mtcars
```
### 3.1.2 Visual Assessment of Cluster Tendency (VAT)

```{r}
fviz_dist(
  dist(mtcars),
  show_labels = FALSE
) + labs(title = "mtcar")
```
The Hopkins statistic of 0.9999998 indicates a strong clustering tendency in the dataset, suggesting that the data is well-suited for cluster analysis. The VAT plot visually reinforces this, showing distinct blocks of similar values along the diagonal, which corresponds to well-separated clusters.


## 3.2 Finding the Optimal Number of Clusters

### 3.2.1 Silhouette Method

```{r}
mtcars_silhouette_hierarchical <- fviz_nbclust(wdbc, hcut, method = "silhouette")
mtcars_silhouette_hierarchical
```

### 3.2.2 Gap Statistics

```{r}
mtcars_gap_hierarchical <- fviz_nbclust(mtcars, hcut, nstart = 50, method = "gap_stat", nboot = 500)
mtcars_gap_hierarchical
```


# 4. Clustering Analysis

## 4.1 Agglomerative Hierarchical Clustering

### 4.1.1  Single Linkage Method

```{r}
res.dist <- dist(mtcars, method = "manhattan")
hc_single <- hclust(d = res.dist, method = "single")

single_dendrogram <- fviz_dend(
  hc_single,
  cex = 0.5, # Increase the label size
  k = 2, # Number of clusters
  k_colors = "jco", # Color palette for clusters
  rect = TRUE,
  rect_border = "jco", # Add rectangle around clusters
  rect_fill = TRUE, # Fill the rectangle
  label_cols = "black", # Color of labels
  label_cex = 0.5, # Font size of labels
  main = "Single Linkage Dendrogram"
) + theme_minimal()
```

```{r}
cor(res.dist, cophenetic(hc_single))
```

### 4.1.2 Complete Linkage Method
```{r}
hc_complete <- hclust(d = res.dist, method = "complete")

complete_dendrogram <- fviz_dend(
  hc_complete,
  cex = 0.5,
  k = 2,
  k_colors = "jco",
  rect = TRUE,
  rect_border = "jco",
  rect_fill = TRUE,
  label_cols = "black",
  label_cex = 0.5,
  main = "Complete Linkage Dendrogram"
) + theme_minimal()
```

```{r}
cor(res.dist, cophenetic(hc_complete))
```

### 4.1.3 Average Linkage Method
```{r}
hc_average <- hclust(d = res.dist, method = "average")

average_dendrogram <- fviz_dend(
  hc_average,
  cex = 0.5,
  k = 2,
  k_colors = "jco",
  rect = TRUE,
  rect_border = "jco",
  rect_fill = TRUE,
  label_cols = "black",
  label_cex = 0.5,
  main = "Average Linkage Dendrogram"
) + theme_minimal()
```


```{r}
cor(res.dist, cophenetic(hc_average))
```

### 4.1.4 Ward Linkage Method

```{r}
hc_ward_d <- hclust(d = res.dist, method = "ward.D")

ward_d_dendrogram <- fviz_dend(
  hc_ward_d,
  cex = 0.5,
  k = 2,
  k_colors = "jco",
  rect = TRUE,
  rect_border = "jco",
  rect_fill = TRUE,
  label_cols = "black",
  label_cex = 0.5,
  main = "Ward D Linkage Dendrogram"
) + theme_minimal()
```


```{r}
cor(res.dist, cophenetic(hc_ward_d))
```

### 4.1.5 Ward.D2 Method

```{r}
hc_ward_d2 <- hclust(d = res.dist, method = "ward.D2")

ward_d2_dendrogram <- fviz_dend(
  hc_ward_d2,
  cex = 0.5,
  k = 2,
  k_colors = "jco",
  rect = TRUE,
  rect_border = "jco",
  rect_fill = TRUE,
  label_cols = "black",
  label_cex = 0.5,
  main = "Ward D2 Linkage Dendrogram"
) + theme_minimal()
```

```{r}
cor(res.dist, cophenetic(hc_ward_d2))
```

## 4.2 Comparing Dendograms

```{r}
dend_complete <- mtcars %>%
  dist %>%
  hclust("complete") %>%
  as.dendrogram
dend_single <- mtcars %>%
  dist %>%
  hclust("single") %>%
  as.dendrogram
dend_average <- mtcars %>%
  dist %>%
  hclust("average") %>%
  as.dendrogram
dend_centroid <- mtcars %>%
  dist %>%
  hclust("centroid") %>%
  as.dendrogram
dend_ward <- mtcars %>%
  dist %>%
  hclust("ward.D") %>%
  as.dendrogram
dend_ward2 <- mtcars %>%
  dist %>%
  hclust("ward.D2") %>%
  as.dendrogram
```


```{r}
dend_list <- dendlist(
  "Complete" = dend_complete,
  "Single" = dend_single,
  "Average" = dend_average,
  "Centroid" = dend_centroid,
  "WardD" = dend_ward,
  "WardD2" = dend_ward2
)

cors <- cor.dendlist(dend_list)

round(cors, 2)
```

```{r}
corrplot(cors, "pie", "lower")
```

# 5. Cluster Validation

## 5.1 Internal Validation
```{r}
intern_mtcars <- clValid(mtcars, 2:6, clMethods = c("hierarchical", "kmeans", "pam"), validation = "internal")

summary(intern_mtcars)
```

```{r}
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))

plot(intern_mtcars, legend = FALSE)

plot(nClusters(intern_mtcars), measures(intern_mtcars, "Dunn")[, , 1], type = "n", axes = FALSE, xlab = "", ylab = "")
legend("center", clusterMethods(intern_mtcars), col = 1:9, lty = 1:9, pch = paste(1:9))
```

## 5.2 Stability Validation

```{r}
stab_mtcars <- clValid(mtcars, 2:6, clMethods = c("hierarchical", "kmeans", "pam"), validation = "stability")

optimal_scores_stab_mtcars <- optimalScores(stab_mtcars)
optimal_scores_stab_mtcars
```

```{r}
op <- par(no.readonly = TRUE)
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))

plot(stab_mtcars, measure = c("APN", "AD", "ADM", "FOM"), legend = F)

plot(nClusters(stab_mtcars), measures(stab_mtcars, "APN")[, , 1], type = "n", axes = F, xlab = "", ylab = "")
legend("center", clusterMethods(stab_mtcars), col = 1:9, lty = 1:9, pch = paste(1:9))

par(op)
```

# 6. Conclusion and Recommendation