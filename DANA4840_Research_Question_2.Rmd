---
title: "DANA 4840 - Research Question 2"
author: "Aryan Mukherjee, Maryam Gadimova, Patricia Tating, Roman Shrestha"
output:
  pdf_document: default
  html_document: default
---

**How can we identify and interpret similarities in features within clustering results to enhance the understanding of cluster formation and structure?**

## Loading the Libraries
```{r, message=FALSE, warning=FALSE}
library("tidyverse")
library("factoextra")
library("dendextend")
library("cluster")
library("gridExtra")
```

## Reading the Data
```{r}
mtcars <- read.csv("data/mtcars.csv", header = T, sep = ",")
head(mtcars)
```

## Checking Data Structure
```{r}
dim(mtcars)
str(mtcars)
```
We can see that our data comprises 32 observations of different car models and 12 automobile features of mixed (numeric, integer, character) data types.


## Data Pre-processing
```{r}
mtcars_categorical <- data.frame(
  cyl = mtcars$cyl,
  vs = mtcars$vs,
  am = mtcars$am,
  gear = mtcars$gear,
  carb = mtcars$carb
)

mtcars_numerical <- data.frame(
  mpg = mtcars$mpg,
  disp = mtcars$disp,
  hp = mtcars$hp,
  drat = mtcars$drat,
  wt = mtcars$wt,
  qsec = mtcars$qsec
)

mtcars_numerical_scaled <- data.frame(scale(mtcars_numerical))

mtcars_joined <- cbind(mtcars_numerical_scaled, mtcars_categorical)
rownames(mtcars_joined) <- mtcars$model
mtcars <- mtcars_joined
head(mtcars)
```
Our numerical features have different scale of measurements, so we standardized the data to ensure each variable contributes equally to the distance calculations, preventing variables with larger scales to have more weight in the clustering results. We also do not standardize the categorical data.


## Average Linkage Method
```{r}
res.dist <- dist(mtcars, method = "manhattan")

hc_average <- hclust(d = res.dist, method = "average")
grp_average <- cutree(hc_average, k = 2)

average_cluster <- fviz_cluster(
  list(data = mtcars, cluster = grp_average),
  palette = "jco",
  geom = "point",
  ellipse.type = "convex",
  show.clust.cent = FALSE,
  main = "Average Linkage Cluster",
  ggtheme = theme_minimal()
)

average_dendrogram <- fviz_dend(
  hc_average,
  cex = 0.5,
  k = 2,
  k_colors = "jco",
  rect = TRUE,
  rect_border = "jco",
  rect_fill = TRUE,
  label_cols = "black",
  label_cex = 0.5,
  main = "Average Linkage Dendrogram",
  ggtheme = theme_minimal()
)
average_dendrogram
```

## Clustering Results
```{r}
grp <- cutree(hc_average, k = 2)
head(grp, n = 32)
```
After applying the average linkage method to cluster the dataset, we obtained the following cluster assignments for each car model. Below is the number of members for each clusters:


```{r}
table(grp)
```

```{r}
fviz_cluster(list(data = mtcars, cluster = grp),
             palette = c("#2E9FDF", "#E7B800"),
             ellipse.type = "convex", #Concentration ellipse
             repel = TRUE, #Avoid label overplotting (slow)
             show.clust.cent = FALSE, ggtheme = theme_minimal()
             )
```
The cluster plot clearly shows how the car models are grouped together. However, it lacks insight into the specific feature similarities within each cluster that led to these groupings. To make our clustering results valuable, we need to interpret these clusters to make actionable decisions, particularly in marketing. By understanding the key features that define each cluster, we can tailor our marketing strategies to target specific customer segments more effectively. For example, if one cluster predominantly includes fuel-efficient cars, we can market these models to environmentally conscious consumers. Similarly, if another cluster consists of high-performance cars, we can focus our marketing efforts on car enthusiasts and performance-focused buyers.


There are several methods to identify and interpret clusters:

**1. Descriptive Statistics**

* Objective: Summarize and describe the features of each cluster.
* Implementation: Mean and Median, Standard Deviation and Range
* Use Case: In customer segmentation, calculate the average age, income, and spending score for each cluster.

**2. Visualization**

* Objective: Provide a graphical representation of feature similarities and differences within clusters.
* Implementation: Box Plots, Heatmaps, Pairwise Scatter Plots
* Use Case: In customer segmentation, visualize the average age, income, and spending score for each cluster.

**3. Feature Importance Analysis**

* Objective: Determine the most significant features contributing to cluster formation.
* Implementation: Decision Trees, Feature Importance Scores, ANOVA 
* Use Case: In a marketing dataset, use a decision tree to identify the key demographic features driving customer segmentation.

**4. Cluster Profiles**

* Objective: Create detailed profiles for each cluster based on feature similarities.
* Implementation: Centroids, Cluster Summaries
* Use Case: In healthcare data, create profiles for patient clusters based on average age, medical history, and treatment outcomes.

**5. Correlation Analysis**

* Objective: Assess the relationships between features within clusters.
* Implementation: Correlation Coefficients, Correlation Matrices
* Use Case: In financial data, analyze the correlation between different financial indicators within investor clusters.


## Visualization on Cluster Features
```{r}
mtcars_num_results <- cbind(
  mtcars_numerical,
  hclust_average = grp
)
```

```{r}
par(mfrow = c(2, 3))

color_palette <- c("#4494a4", "#7ca454", "#f9d448", "#9fc4b7", "#fcea9e", "#a6ccd4")

mtcars_results_col <- colnames(mtcars_num_results)[-length(colnames(mtcars_num_results))]

for (i in seq_along(mtcars_results_col)) {
  column_name <- mtcars_results_col[i]

  boxplot(
    mtcars_num_results[[column_name]] ~ mtcars_num_results$hclust_average,
    xlab = "H Cluster",
    ylab = column_name,
    main = paste(column_name),
    col = color_palette[i %% length(color_palette) + 1]
  )
}

par(mfrow = c(1, 1))
```
```{r}
mtcars_cat_results <- cbind(
  mtcars_categorical,
  hclust_average = grp
)
```

```{r}
mtcars_cat_results$hclust_average <- as.factor(mtcars_cat_results$hclust_average)

cols_palette <- c("#4494a4", "#f9d448")
variables_to_plot <- colnames(mtcars_cat_results)[colnames(mtcars_cat_results) != "hclust_average"]

plot_cyl <- ggplot(mtcars_cat_results, aes(x = factor(cyl), fill = factor(hclust_average))) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = sample(cols_palette)) +
  theme_classic() +
  ggtitle("Cylinders")

plot_vs <- ggplot(mtcars_cat_results, aes(x = factor(vs), fill = factor(hclust_average))) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = sample(cols_palette)) +
  theme_classic() +
  ggtitle("Engine Shape (vs)")

plot_am <- ggplot(mtcars_cat_results, aes(x = factor(am), fill = factor(hclust_average))) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = sample(cols_palette)) +
  theme_classic() +
  ggtitle("Transmission (am)")

plot_gear <- ggplot(mtcars_cat_results, aes(x = factor(gear), fill = factor(hclust_average))) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = sample(cols_palette)) +
  theme_classic() +
  ggtitle("Gears")

plot_carb <- ggplot(mtcars_cat_results, aes(x = factor(carb), fill = factor(hclust_average))) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = sample(cols_palette)) +
  theme_classic() +
  ggtitle("Carburetors")

# Arrange the plots in a grid
grid.arrange(plot_cyl, plot_vs, plot_am, plot_gear, plot_carb, ncol = 2)

```
We can use box plots and bar plots to visualize the distinct characteristics of each cluster based on the features. This visualization clearly highlights the differences between the clusters. However, we'll have to validate this with statistics as sometimes visualization can be subjective.


## Descriptive Statistics on Clusters
```{r}
summary(mtcars_num_results[mtcars_num_results$hclust == 1, 1:5])
```
```{r}
summary(mtcars_num_results[mtcars_num_results$hclust == 2, 1:5])
```

* **Cluster 1:** Contains cars that are generally more fuel-efficient, lighter, and have smaller engine displacements and lower horsepower. This cluster likely represents smaller, more economical vehicles.
* **Cluster 2:** Contains cars with lower fuel efficiency, heavier weights, larger engine displacements, and higher horsepower. This cluster likely represents larger, more powerful vehicles.


```{r}
summary(mtcars_cat_results[mtcars_num_results$hclust == 1, 1:5])
```
```{r}
summary(mtcars_cat_results[mtcars_num_results$hclust == 2, 1:5])
```

* **Cluster 1:** Contains cars with fewer cylinders (mostly 4, some 6), a mix of engine shapes (more straight engines), a higher proportion of manual transmissions, typically 4 gears, and fewer carburetors. These cars are generally lighter and more fuel-efficient, indicating they might be more economical or everyday vehicles.
* **Cluster 2:** Consists exclusively of cars with 8 cylinders, V-shaped engines, mostly automatic transmissions, typically 3 gears, and more carburetors. These cars are heavier and less fuel-efficient, indicating they might be more powerful or performance-oriented vehicles.


## Cluster Interpretation

**Cluster 1:** 

Cars in this cluster typically have:

* Lower horsepower (hp)
* Higher fuel efficiency (mpg)
* Lower weight (wt)
* Smaller engine displacement (disp)
* Fewer cylinders (cyl)

These characteristics suggest that cars in Cluster 1 are generally more economical and smaller, possibly including compact and subcompact cars.

**Cluster 2:** 

Cars in this cluster typically have:

* Higher horsepower (hp)
* Lower fuel efficiency (mpg)
* Higher weight (wt)
* Larger engine displacement (disp)
* More cylinders (cyl)

These characteristics suggest that cars in Cluster 2 are generally more powerful and larger, possibly including sports cars, muscle cars, and luxury cars.