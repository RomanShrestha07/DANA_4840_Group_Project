---
title: "DANA4840 Project - Partitional Clustering"
author: "M.Gadimova, A.Mukherjee, R.Shrestha, P.Tating"
date: "`r Sys.Date()`"
output: html_document
---

# 1. Research Statement on Breast Cancer Dataset

Breast cancer is a critical health issue, with early and accurate detection playing a vital role in treatment and patient outcomes. This dataset captures the features of cell nuclei through comprehensive measurements taken during breast cancer biopsies. Each observation spans several measurement times and includes characteristics like radius, texture, perimeter, area, and others. Additionally, labels describing the tumor's malignancy or benignity are included in the dataset.

K-Means and Partitioning Around Medoids (PAM) clustering methods will be used to examine and comprehend the underlying patterns in this data. These techniques aim to segment the dataset into clusters, revealing potential subtypes and characteristics of the disease. This clustering analysis not only provides insights into the heterogeneity of breast cancer but also aids in identifying key features that distinguish between benign and malignant cases. The findings can contribute to improving diagnostic accuracy and personalized treatment approaches.

# 2. Preliminaries

Before diving into the cluster analysis, let's first thoroughly examine and understand our data. This preliminary step will allow us to identify key patterns and characteristics within the dataset, ensuring a solid foundation for accurate analysis. By doing so, we can address any potential data quality issues and refine our approach for more meaningful results.


```{r}
library("tidyverse")
library("factoextra")
library("dendextend")
library("hopkins")
library("corrplot")
library("cluster")
library("patchwork")
library("clValid")

set.seed(101)
```

### 2.1 Reading the Data

```{r}
wdbc <- read.table("data/wdbc.data", header = T, sep = ",")
head(wdbc)
```
```{r}
correct_column_names <- c(
  "ID", "Diagnosis", "radius1", "texture1",
  "perimeter1", "area1", "smoothness1", "compactness1",
  "concavity1", "concave_points1", "symmetry1", "fractal_dimension1",
  "radius2", "texture2", "perimeter2", "area2",
  "smoothness2", "compactness2", "concavity2", "concave_points2",
  "symmetry2", "fractal_dimension2", "radius3", "texture3",
  "perimeter3", "area3", "smoothness3", "compactness3",
  "concavity3", "concave_points3", "symmetry3", "fractal_dimension3"
)

colnames(wdbc) <- correct_column_names
head(wdbc)
```

### 2.1.2. Checking the internal structure: wdbc

```{r}
dim(wdbc)
str(wdbc)

```

#### 2.1.3. Pre-processing and Normalizing: wdbc
```{r}
diagnosis <- wdbc$diagnosis
wdbc_numerical <- wdbc[, -c(1, 2)]

wdbc_scaled <- data.frame(scale(wdbc_numerical))
rownames(wdbc_scaled) <- wdbc$ID
wdbc <- wdbc_scaled
head(wdbc)

```





## 2.3 Feature Explanation

## 2.4 Exploratory Data Analysis
#### 2.4.2 Visualize missing values
```{r}
missing_wdbc <- sapply(wdbc, function(x) sum(is.na(x)))
missing_wdbc
```

#### 2.4.3 Boxplots for different feature groups
```{r}
mean_columns <- grep("_mean", names(wdbc), value = TRUE)
se_columns <- grep("_se", names(wdbc), value = TRUE)
worst_columns <- grep("_worst", names(wdbc), value = TRUE)

boxplot(wdbc[, mean_columns], horizontal = T)
boxplot(wdbc[, se_columns], horizontal = T)
boxplot(wdbc[, worst_columns], horizontal = T)
```

#### 2.4.4 Pie chart for diagnosis distribution
```{r}
diagnosis_freq <- table(diagnosis)
diagnosis_rel_freq <- prop.table(diagnosis_freq) * 100
pie(diagnosis_rel_freq, main = "% Distribution of Benign/Malignant Cancer", col = c("limegreen", "red2"))
```


## 2.5 Data Pre-processing


```{r}
wdbc_numerical <- wdbc[, -c(1, 2)]

wdbc_scaled <- data.frame(scale(wdbc_numerical))
rownames(wdbc_scaled) <- wdbc$ID
wdbc <- wdbc_scaled
head(wdbc)
```

# 3. Pre-clustering Assessment

Before performing clustering analysis, it is crucial to conduct a pre-clustering assessment to evaluate the dataset's cluster tendency and determine the optimal clustering approach. Tools like the Hopkins statistic and VAT can help assess whether the data points possess significant clustering tendencies. Once cluster tendency is established, the next step involves finding the optimal number of clusters. This can be achieved using methods such as the Elbow Method, Silhouette Analysis, or the Gap Statistic, each providing insights into the most meaningful way to partition the data. 

## 3.1 Assessing Cluster Tendency

```{r}
fviz_pca_ind(
  prcomp(wdbc),
  title = "PCA - wdbc",
  palette = "jco",
  geom = "point",
  ggtheme = theme_classic(),
  legend = "bottom"
)
```

### 3.1.1 Hopkins Statistics

```{r}
set.seed(25)

hopkins_wdbc <- hopkins(wdbc, m = ceiling(nrow(wdbc) / 10))
hopkins_wdbc
```

### 3.1.2 Visual Assessment of Cluster Tendency (VAT)

```{r}
fviz_dist(
  dist(wdbc),
  show_labels = FALSE
) + labs(title = "wdbc")
```

## 3.2 Finding the Optimal Number of Clusters

### 3.2.1 Elbow Method

```{r}
wdbc_elbow_kmeans <- fviz_nbclust(wdbc, kmeans, method = "wss") + geom_vline(xintercept = 2, linetype = 2)
wdbc_elbow_kmeans
```

### 3.2.2 Silhouette Method

```{r}
wdbc_silhouette_kmeans <- fviz_nbclust(wdbc, kmeans, method = "silhouette")
wdbc_silhouette_pam <- fviz_nbclust(wdbc, pam, method = "silhouette")

wdbc_silhouette_kmeans +
  wdbc_silhouette_pam +
  plot_layout(ncol = 2)
```

### 3.2.3 Gap Statistics
```{r}
# Uncomment to run Gap Statistic method
# wdbc_gap_kmeans <- fviz_nbclust(wdbc, kmeans, nstart = 50, method = "gap_stat", nboot = 500)
# wdbc_gap_pam <- fviz_nbclust(wdbc, pam, nstart = 50, method = "gap_stat", nboot = 800)
# 
# wdbc_gap_kmeans +
#   wdbc_gap_pam +
#   plot_layout(ncol = 2)
```

### 3.1.4. TODO: the positive negative bar plot


# 4. Clustering Analysis

## 4.1 K-Means Clustering 
```{r}
set.seed(101)

km.res <- kmeans(wdbc, centers = 2, nstart = 100)

kmeans_graph <- fviz_cluster(
  km.res,
  data = wdbc,
  palette = c("red", "blue"),
  ellipse.type = "convex",
  star.plot = TRUE,
  ellipse = TRUE,
  geom = "point",
  main = "K-Means Clustering Results",
  ggtheme = theme_minimal()
)
kmeans_graph
```

## 4.2 Partitional Around Medoid (PAM) Clustering 
```{r}
set.seed(101)

pam.res <- pam(wdbc, k = 2)

pam_graph <- fviz_cluster(
  pam.res,
  data = wdbc,
  palette = c("red", "blue"),
  ellipse.type = "convex",
  star.plot = TRUE,
  ellipse = TRUE,
  geom = "point",
  main = "PAM Clustering Results",
  ggtheme = theme_minimal()
)
pam_graph
```

## 4.3 Comparing K-Means and PAM
```{r}
kmeans_graph + pam_graph + plot_layout(ncol = 1)

```

# 5. Cluster Validation

## 5.1 External Validation

## 5.2 Internal Validation

### 4.4. Internal Validation of K-means and PAM
```{r}
intern_wdbc <- clValid(wdbc, 2:6, clMethods = c("kmeans", "hierarchical", "pam"), validation = "internal")

summary(intern_wdbc)
```
```{r}
op <- par(no.readonly = TRUE)
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))

plot(intern_wdbc, legend = FALSE)

plot(nClusters(intern_wdbc), measures(intern_wdbc, "Dunn")[, , 1], type = "n", axes = FALSE, xlab = "", ylab = "")
legend("center", clusterMethods(intern_wdbc), col = 1:9, lty = 1:9, pch = paste(1:9))

par(op)
```

## 5.3 Stability Validation
```{r}
stab_wdbc <- clValid(wdbc, nClust = 2:6, clMethods = c("hierarchical", "kmeans", "pam"), validation = "stability")

optimal_scores_stab <- optimalScores(stab_wdbc)
optimal_scores_stab
```

```{r}
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))

plot(stab_wdbc, measure = c("APN", "AD", "ADM", "FOM"), legend = FALSE)

plot(nClusters(stab_wdbc), measures(stab_wdbc, "APN")[, , 1], type = "n", axes = FALSE, xlab = "", ylab = "")
legend("left", clusterMethods(stab_wdbc), col = 1:9, lty = 1:9, pch = paste(1:9))
```
# 6. Conclusion and Recommendation























