<<<<<<< HEAD
---
title: "DANA4840 Project - Partitional Clustering"
author: "M.Gadimova, A.Mukherjee, R.Shrestha, P.Tating"
date: "`r Sys.Date()`"
output: html_document
---

# 1. Research Statement on Breast Cancer Dataset

Breast cancer is a critical health issue, with early and accurate detection playing a vital role in treatment and patient outcomes. This dataset captures the features of cell nuclei through comprehensive measurements taken during breast cancer biopsies. Each observation spans several measurement times and includes characteristics like radius, texture, perimeter, area, and others. Additionally, labels describing the tumor's malignancy or benignity are included in the dataset.

K-Means and Partitioning Around Medoids (PAM) clustering methods will be used to examine and comprehend the underlying patterns in this data. These techniques aim to segment the dataset into clusters, revealing potential subtypes and characteristics of the disease. This clustering analysis not only provides insights into the heterogeneity of breast cancer but also aids in identifying key features that distinguish between benign and malignant cases. The findings can contribute to improving diagnostic accuracy and personalized treatment approaches.

# 2. Preliminaries

Before diving into the cluster analysis, let's first thoroughly examine and understand our data. This preliminary step will allow us to identify key patterns and characteristics within the dataset, ensuring a solid foundation for accurate analysis. By doing so, we can address any potential data quality issues and refine our approach for more meaningful results.


```{r}
library("tidyverse")
library("factoextra")
library("dendextend")
library("hopkins")
library("corrplot")
library("cluster")
library("patchwork")
library("clValid")

set.seed(101)
```

### 2.1 Reading the Data

```{r}
wdbc <- read.table("data/wdbc.data", header = T, sep = ",")
head(wdbc)
```
```{r}
correct_column_names <- c(
  "ID", "diagnosis", "radius1", "texture1",
  "perimeter1", "area1", "smoothness1", "compactness1",
  "concavity1", "concave_points1", "symmetry1", "fractal_dimension1",
  "radius2", "texture2", "perimeter2", "area2",
  "smoothness2", "compactness2", "concavity2", "concave_points2",
  "symmetry2", "fractal_dimension2", "radius3", "texture3",
  "perimeter3", "area3", "smoothness3", "compactness3",
  "concavity3", "concave_points3", "symmetry3", "fractal_dimension3"
)

colnames(wdbc) <- correct_column_names
head(wdbc)
```

### 2.1.2. Checking the internal structure: wdbc

```{r}
dim(wdbc)
str(wdbc)

```

#### 2.1.3. Pre-processing and Normalizing: wdbc
```{r}
diagnosis <- wdbc$diagnosis
wdbc_numerical <- wdbc[, -c(1, 2)]

wdbc_scaled <- data.frame(scale(wdbc_numerical))
rownames(wdbc_scaled) <- wdbc$ID
wdbc <- wdbc_scaled
head(wdbc)

```





## 2.3 Feature Explanation

## 2.4 Exploratory Data Analysis
#### 2.4.2 Visualize missing values
```{r}
missing_wdbc <- sapply(wdbc, function(x) sum(is.na(x)))
missing_wdbc
```

#### 2.4.3 Boxplots for different feature groups
```{r}
mean_columns <- grep("_mean", names(wdbc), value = TRUE)
se_columns <- grep("_se", names(wdbc), value = TRUE)
worst_columns <- grep("_worst", names(wdbc), value = TRUE)


boxplot(wdbc[, mean_columns], horizontal = T)
boxplot(wdbc[, se_columns], horizontal = T)
boxplot(wdbc[, worst_columns], horizontal = T)
```

#### 2.4.4 Pie chart for diagnosis distribution
```{r}

diagnosis_freq <- table(diagnosis)
diagnosis_rel_freq <- prop.table(diagnosis_freq) * 100
diagnosis_rel_freq
labels <- paste0(names(diagnosis_rel_freq), ": ", round(diagnosis_rel_freq, 1), "%")

pie(diagnosis_rel_freq,
    labels = labels,
    main = "% Distribution of Benign/Malignant Cancer",
    col = c("limegreen", "red2"))

```
#### Benign cancer makes up approximately 62.9% of the dataset, while Malignant cancer constitutes about 37.1%. Benign cancer cases are approximately 1.69 times more prevalent than Malignant cancer cases in the dataset.

## 2.5 Data Pre-processing


```{r}
wdbc_numerical <- wdbc[, -c(1, 2)]

wdbc_scaled <- data.frame(scale(wdbc_numerical))
rownames(wdbc_scaled) <- wdbc$ID
wdbc <- wdbc_scaled
head(wdbc)
```

# 3. Pre-clustering Assessment

Before performing clustering analysis, it is crucial to conduct a pre-clustering assessment to evaluate the dataset's cluster tendency and determine the optimal clustering approach. Tools like the Hopkins statistic and VAT can help assess whether the data points possess significant clustering tendencies. Once cluster tendency is established, the next step involves finding the optimal number of clusters. This can be achieved using methods such as the Elbow Method, Silhouette Analysis, or the Gap Statistic, each providing insights into the most meaningful way to partition the data. 

## 3.1 Assessing Cluster Tendency

```{r}
fviz_pca_ind(
  prcomp(wdbc),
  title = "PCA - wdbc",
  palette = "jco",
  geom = "point",
  ggtheme = theme_classic(),
  legend = "bottom"
)
```

### 3.1.1 Hopkins Statistics

```{r}
set.seed(25)

hopkins_wdbc <- hopkins(wdbc, m = ceiling(nrow(wdbc) / 10))
hopkins_wdbc
```
#### A Hopkins statistic value of 1 indicates that the dataset exhibits a high degree of clusterability

### 3.1.2 Visual Assessment of Cluster Tendency (VAT)

```{r}
fviz_dist(
  dist(wdbc),
  show_labels = FALSE
) + labs(title = "wdbc")
```

#### We can see at least two clusters visually

## 3.2 Finding the Optimal Number of Clusters

### 3.2.1 Elbow Method

```{r}
wdbc_elbow_kmeans <- fviz_nbclust(wdbc, kmeans, method = "wss") + geom_vline(xintercept = 2, linetype = 2)
wdbc_elbow_kmeans
```
#### Optimal number of clusters according to elbow method seems to be 2

### 3.2.2 Silhouette Method

```{r}
wdbc_silhouette_kmeans <- fviz_nbclust(wdbc, kmeans, method = "silhouette")
wdbc_silhouette_pam <- fviz_nbclust(wdbc, pam, method = "silhouette")

wdbc_silhouette_kmeans +
  wdbc_silhouette_pam +
  plot_layout(ncol = 2)
```

#### Based on the silhouette analysis performed on both K-means and PAM clustering methods, the optimal number of clusters is determined to be 2. his conclusion is supported by the highest silhouette width observed at k=2

### 3.2.3 Gap Statistics
```{r}

wdbc_gap_kmeans <- fviz_nbclust(wdbc, kmeans, nstart = 50, method = "gap_stat", nboot = 500)
# wdbc_gap_pam <- fviz_nbclust(wdbc, pam, nstart = 50, method = "gap_stat", nboot = 800)  # dont run it

# wdbc_gap_kmeans +
#   wdbc_gap_pam +
#   plot_layout(ncol = 2)

wdbc_gap_kmeans
```
#### The Gap Statistic graph indicates that the optimal number of clusters is k=2, this conclusion is drawn from observing the most significant change in the Gap Statistic value at this point.

### 3.1.4. TODO: the positive negative bar plot


# 4. Clustering Analysis

## 4.1 K-Means Clustering 
```{r}
set.seed(101)

km.res <- kmeans(wdbc, centers = 2, nstart = 100)

kmeans_graph <- fviz_cluster(
  km.res,
  data = wdbc,
  palette = c("red", "blue"),
  ellipse.type = "convex",
  star.plot = TRUE,
  ellipse = TRUE,
  geom = "point",
  main = "K-Means Clustering Results",
  ggtheme = theme_minimal()
)
kmeans_graph
```

#### The K-Means algorithm has identified two distinct clusters in the data. 

## 4.2 Partitional Around Medoid (PAM) Clustering 
```{r}
set.seed(101)

pam.res <- pam(wdbc, k = 2)

pam_graph <- fviz_cluster(
  pam.res,
  data = wdbc,
  palette = c("red", "blue"),
  ellipse.type = "convex",
  star.plot = TRUE,
  ellipse = TRUE,
  geom = "point",
  main = "PAM Clustering Results",
  ggtheme = theme_minimal()
)
pam_graph
```

#### The PAM algorithm has identified two distinct clusters in the data. In this graph, the medoids are likely represented by the point where the lines within each cluster converge.
####PAM clustering algorithm has effectively segregated the data points into two distinct groups based on their similarity

## 4.3 Comparing K-Means and PAM
```{r}
kmeans_graph + pam_graph + plot_layout(ncol = 1)

```
#### Both K-Means and PAM have provided similar clustering results for this dataset but, the PAM clustering results show a little bit of overlap between both clusters, which is not preferable. THerefore, k-means seems to be the better clustering method for this dataset
# 5. Cluster Validation

## 5.1 External Validation

## 5.2 Internal Validation

### 4.4. Internal Validation of K-means and PAM
```{r}
intern_wdbc <- clValid(wdbc, 2:6, clMethods = c("kmeans", "hierarchical", "pam"), validation = "internal")

summary(intern_wdbc)
```

#### Connectivity: The optimal score of 6.7313 is achieved with Hierarchical clustering at 2 clusters. This indicates the most compact clustering with minimal inter-cluster distances.
#### Dunn Index: The highest Dunn index of 0.3835 is observed with Hierarchical clustering at 3 clusters, suggesting the best separation between clusters.
#### Silhouette Width: The maximum silhouette width of 0.6430 is found with Hierarchical clustering at 2 clusters, reflecting the highest average similarity within clusters and dissimilarity between clusters.

#### Hierarchical clustering with 2 clusters is generally optimal, especially for Connectivity and Silhouette width, while 3 clusters are preferred for the Dunn index.

```{r}
op <- par(no.readonly = TRUE)
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))

plot(intern_wdbc, legend = FALSE)

plot(nClusters(intern_wdbc), measures(intern_wdbc, "Dunn")[, , 1], type = "n", axes = FALSE, xlab = "", ylab = "")
legend("center", clusterMethods(intern_wdbc), col = 1:9, lty = 1:9, pch = paste(1:9))

par(op)
```

#### Connectivity: The connectivity graph shows us that hierarchical is the best method of evaluation with consistent, lowest values
#### The Dunn index also shows that hierarchical is the best clustering method as it's values are consistently high
#### The Silhouette method also shows hierarchical as the best method with consistently higher values

## 5.3 Stability Validation
```{r}
stab_wdbc <- clValid(wdbc_scaled, nClust = 2:6, clMethods = c("hierarchical", "kmeans", "pam"), validation = "stability")

optimal_scores_stab <- optimalScores(stab_wdbc)
optimal_scores_stab
```

#### APN (Average Path Length): The lowest APN of 0.0005658954 is achieved with Hierarchical clustering at 3 clusters, indicating minimal average path lengths among clusters.
#### AD (Average Distance): The highest AD of 4.8239854590 is found with PAM clustering at 6 clusters, reflecting the average distance within clusters.
#### ADM (Average Dissimilarity): The lowest ADM of 0.0179680671 is achieved with Hierarchical clustering at 3 clusters, showing minimal average dissimilarity within clusters.
#### FOM (Freeman's Measure): The highest FOM of 0.7178725784 is observed with PAM clustering at 6 clusters, suggesting better clustering performance according to Freeman’s measure.
#### Hierarchical Clustering with 3 Clusters: Optimal for APN and ADM, indicating compact and well-defined clusters with minimal average path length and dissimilarity.
#### PAM Clustering with 6 Clusters: Optimal for AD and FOM, reflecting more spread-out clusters and superior overall clustering performance based on Freeman’s Measure.

```{r}
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))

plot(stab_wdbc, measure = c("APN", "AD", "ADM", "FOM"), legend = FALSE)

plot(nClusters(stab_wdbc), measures(stab_wdbc, "APN")[, , 1], type = "n", axes = FALSE, xlab = "", ylab = "")
legend("left", clusterMethods(stab_wdbc), col = 1:9, lty = 1:9, pch = paste(1:9))
```

#### Hierarchical shows best results in all 4 measures with being consistently high in AD anf FOM, while being low in both ADM and APN

# 6. Conclusion and Recommendation























=======
---
title: "DANA4840 Project - Partitional Clustering"
author: "M.Gadimova, A.Mukherjee, R.Shrestha, P.Tating"
date: "`r Sys.Date()`"
output: html_document
---

# 1. Research Statement on Breast Cancer Dataset

Breast cancer is a critical health issue, with early and accurate detection playing a vital role in treatment and patient outcomes. This dataset captures the features of cell nuclei through comprehensive measurements taken during breast cancer biopsies. Each observation spans several measurement times and includes characteristics like radius, texture, perimeter, area, and others. Additionally, labels describing the tumor's malignancy or benignity are included in the dataset.

K-Means and Partitioning Around Medoids (PAM) clustering methods will be used to examine and comprehend the underlying patterns in this data. These techniques aim to segment the dataset into clusters, revealing potential subtypes and characteristics of the disease. This clustering analysis not only provides insights into the heterogeneity of breast cancer but also aids in identifying key features that distinguish between benign and malignant cases. The findings can contribute to improving diagnostic accuracy and personalized treatment approaches.

# 2. Preliminaries

Before diving into the cluster analysis, let's first thoroughly examine and understand our data. This preliminary step will allow us to identify key patterns and characteristics within the dataset, ensuring a solid foundation for accurate analysis. By doing so, we can address any potential data quality issues and refine our approach for more meaningful results.


```{r}
library("tidyverse")
library("factoextra")
library("dendextend")
library("hopkins")
library("corrplot")
library("cluster")
library("patchwork")
library("clValid")

set.seed(101)
```

### 2.1 Reading the Data

```{r}
wdbc <- read.table("data/wdbc.data", header = T, sep = ",")
head(wdbc)
```
```{r}
correct_column_names <- c(
  "ID", "Diagnosis", "radius1", "texture1",
  "perimeter1", "area1", "smoothness1", "compactness1",
  "concavity1", "concave_points1", "symmetry1", "fractal_dimension1",
  "radius2", "texture2", "perimeter2", "area2",
  "smoothness2", "compactness2", "concavity2", "concave_points2",
  "symmetry2", "fractal_dimension2", "radius3", "texture3",
  "perimeter3", "area3", "smoothness3", "compactness3",
  "concavity3", "concave_points3", "symmetry3", "fractal_dimension3"
)

colnames(wdbc) <- correct_column_names
head(wdbc)
```

### 2.1.2. Checking the internal structure: wdbc

```{r}
dim(wdbc)
str(wdbc)

```

#### 2.1.3. Pre-processing and Normalizing: wdbc
```{r}
diagnosis <- wdbc$diagnosis
wdbc_numerical <- wdbc[, -c(1, 2)]

wdbc_scaled <- data.frame(scale(wdbc_numerical))
rownames(wdbc_scaled) <- wdbc$ID
wdbc <- wdbc_scaled
head(wdbc)

```





## 2.3 Feature Explanation

## 2.4 Exploratory Data Analysis
#### 2.4.2 Visualize missing values
```{r}
missing_wdbc <- sapply(wdbc, function(x) sum(is.na(x)))
missing_wdbc
```

#### 2.4.3 Boxplots for different feature groups
```{r}
mean_columns <- grep("_mean", names(wdbc), value = TRUE)
se_columns <- grep("_se", names(wdbc), value = TRUE)
worst_columns <- grep("_worst", names(wdbc), value = TRUE)

boxplot(wdbc[, mean_columns], horizontal = T)
boxplot(wdbc[, se_columns], horizontal = T)
boxplot(wdbc[, worst_columns], horizontal = T)
```

#### 2.4.4 Pie chart for diagnosis distribution
```{r}
diagnosis_freq <- table(diagnosis)
diagnosis_rel_freq <- prop.table(diagnosis_freq) * 100
pie(diagnosis_rel_freq, main = "% Distribution of Benign/Malignant Cancer", col = c("limegreen", "red2"))
```


## 2.5 Data Pre-processing


```{r}
wdbc_numerical <- wdbc[, -c(1, 2)]

wdbc_scaled <- data.frame(scale(wdbc_numerical))
rownames(wdbc_scaled) <- wdbc$ID
wdbc <- wdbc_scaled
head(wdbc)
```

# 3. Pre-clustering Assessment

Before performing clustering analysis, it is crucial to conduct a pre-clustering assessment to evaluate the dataset's cluster tendency and determine the optimal clustering approach. Tools like the Hopkins statistic and VAT can help assess whether the data points possess significant clustering tendencies. Once cluster tendency is established, the next step involves finding the optimal number of clusters. This can be achieved using methods such as the Elbow Method, Silhouette Analysis, or the Gap Statistic, each providing insights into the most meaningful way to partition the data. 

## 3.1 Assessing Cluster Tendency

```{r}
fviz_pca_ind(
  prcomp(wdbc),
  title = "PCA - wdbc",
  palette = "jco",
  geom = "point",
  ggtheme = theme_classic(),
  legend = "bottom"
)
```

### 3.1.1 Hopkins Statistics

```{r}
set.seed(25)

hopkins_wdbc <- hopkins(wdbc, m = ceiling(nrow(wdbc) / 10))
hopkins_wdbc
```

### 3.1.2 Visual Assessment of Cluster Tendency (VAT)

```{r}
fviz_dist(
  dist(wdbc),
  show_labels = FALSE
) + labs(title = "wdbc")
```

## 3.2 Finding the Optimal Number of Clusters

### 3.2.1 Elbow Method

```{r}
wdbc_elbow_kmeans <- fviz_nbclust(wdbc, kmeans, method = "wss") + geom_vline(xintercept = 2, linetype = 2)
wdbc_elbow_kmeans
```

### 3.2.2 Silhouette Method

```{r}
wdbc_silhouette_kmeans <- fviz_nbclust(wdbc, kmeans, method = "silhouette")
wdbc_silhouette_pam <- fviz_nbclust(wdbc, pam, method = "silhouette")

wdbc_silhouette_kmeans +
  wdbc_silhouette_pam +
  plot_layout(ncol = 2)
```

### 3.2.3 Gap Statistics
```{r}
# Uncomment to run Gap Statistic method
# wdbc_gap_kmeans <- fviz_nbclust(wdbc, kmeans, nstart = 50, method = "gap_stat", nboot = 500)
# wdbc_gap_pam <- fviz_nbclust(wdbc, pam, nstart = 50, method = "gap_stat", nboot = 800)
# 
# wdbc_gap_kmeans +
#   wdbc_gap_pam +
#   plot_layout(ncol = 2)
```

### 3.1.4. TODO: the positive negative bar plot


# 4. Clustering Analysis

## 4.1 K-Means Clustering 
```{r}
set.seed(101)

km.res <- kmeans(wdbc, centers = 2, nstart = 100)

kmeans_graph <- fviz_cluster(
  km.res,
  data = wdbc,
  palette = c("red", "blue"),
  ellipse.type = "convex",
  star.plot = TRUE,
  ellipse = TRUE,
  geom = "point",
  main = "K-Means Clustering Results",
  ggtheme = theme_minimal()
)
kmeans_graph
```

## 4.2 Partitional Around Medoid (PAM) Clustering 
```{r}
set.seed(101)

pam.res <- pam(wdbc, k = 2)

pam_graph <- fviz_cluster(
  pam.res,
  data = wdbc,
  palette = c("red", "blue"),
  ellipse.type = "convex",
  star.plot = TRUE,
  ellipse = TRUE,
  geom = "point",
  main = "PAM Clustering Results",
  ggtheme = theme_minimal()
)
pam_graph
```

## 4.3 Comparing K-Means and PAM
```{r}
kmeans_graph + pam_graph + plot_layout(ncol = 1)

```

# 5. Cluster Validation

## 5.1 External Validation

## 5.2 Internal Validation

### 4.4. Internal Validation of K-means and PAM
```{r}
intern_wdbc <- clValid(wdbc, 2:6, clMethods = c("kmeans", "hierarchical", "pam"), validation = "internal")

summary(intern_wdbc)
```
```{r}
op <- par(no.readonly = TRUE)
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))

plot(intern_wdbc, legend = FALSE)

plot(nClusters(intern_wdbc), measures(intern_wdbc, "Dunn")[, , 1], type = "n", axes = FALSE, xlab = "", ylab = "")
legend("center", clusterMethods(intern_wdbc), col = 1:9, lty = 1:9, pch = paste(1:9))

par(op)
```

## 5.3 Stability Validation
```{r}
stab_wdbc <- clValid(wdbc, nClust = 2:6, clMethods = c("hierarchical", "kmeans", "pam"), validation = "stability")

optimal_scores_stab <- optimalScores(stab_wdbc)
optimal_scores_stab
```

```{r}
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))

plot(stab_wdbc, measure = c("APN", "AD", "ADM", "FOM"), legend = FALSE)

plot(nClusters(stab_wdbc), measures(stab_wdbc, "APN")[, , 1], type = "n", axes = FALSE, xlab = "", ylab = "")
legend("left", clusterMethods(stab_wdbc), col = 1:9, lty = 1:9, pch = paste(1:9))
```
# 6. Conclusion and Recommendation























>>>>>>> a1bfa5c6e1fc8adc5551a3faa0d470a36300cc8b
