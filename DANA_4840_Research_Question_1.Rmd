---
title: "DANA 4840 Project - Research Question 1"
author: "Aryan Mukherjee, Maryam Gadimova, Patricia Tating, Roman Shrestha"
output:
  pdf_document: default
  html_document: default
---

**What is the role of dimensionality reduction techniques such as Principal Component Analysis (PCA), Independent Component Analysis (ICA), t-Distributed Stochastic Neighbor Embedding (t-SNE) or Uniform Manifold Approximation and Projection (UMAP) in enhancing clustering performance?**

## Loading the libraries
```{r loading_libraries, message = FALSE}
library("ggplot2")
library("factoextra")
library("dendextend")
library("hopkins")
library("corrplot")
library("cluster")
library("patchwork")
library("clValid")
library("EMCluster")
library("fastICA")
library("Rtsne")
library("umap")
library("mclust")
library("fpc")
```


## Loading the Dataset
```{r}
wdbc <- read.csv("./data/wdbc.csv", header = T, sep = ",")
head(wdbc)
```


## Pre-processing and Normalizing
```{r}
color_palette <- rainbow(10) #color palette
diagnosis <- wdbc$diagnosis
encoded_diagnosis <- ifelse(diagnosis == "M", 1, 2) #making diagnoses numerical

wdbc_numerical <- wdbc[, -c(1, 2)] #remove id and diagnosis

wdbc_scaled <- data.frame(scale(wdbc_numerical)) #scale data
rownames(wdbc_scaled) <- wdbc$ID
wdbc <- wdbc_scaled
head(wdbc)
dim(wdbc)
```


## K-Means Graph
```{r}
get_kmeans_plot <- function(km.res, data, name) {
  p <- fviz_cluster(
    km.res,
    data = data,
    palette = color_palette,
    ellipse.type = "convex",
    star.plot = TRUE,
    ellipse = TRUE,
    geom = "point",
    main = paste0(name, " K-Means Cluster Plot"),
    ggtheme = theme_minimal()
  )

  return(p)
}
```


## Base
```{r}
set.seed(101)

km.res <- kmeans(wdbc, 2, nstart = 100)

get_kmeans_plot(km.res, wdbc, "Base")
RRand(encoded_diagnosis, km.res$cluster)
```
The clusters are reasonably well-separated with no overlap, and with an Adjusted Rand Index of 0.6707 indicates a good level of agreement with the actual results.


## PCA
```{r}
set.seed(101)

pca_wdbc <- prcomp(wdbc)
pca_index <- which(cumsum(summary(pca_wdbc)$importance[2,]) >= 0.8)[1] # taking principal components which explain 80% of the variation in the data

pca_data <- data.frame(pca_wdbc$x)
pca_data_no <- pca_data[, 1:pca_index]

pca_km.res <- kmeans(pca_data_no, 2, nstart = 100)

get_kmeans_plot(pca_km.res, pca_data_no, "PCA")
RRand(encoded_diagnosis, pca_km.res$cluster)
```
Visually PCA provides clusters that are not very well-separated and also seem to have slight overlap. Even when the number of dimensions are dropped from 30 down to 5, looking at the Adjusted Rand Index of 0.6707 it shows good level of agreement with the actual results. We are reducing the dimensionality (and complexity) while maintaining the same results.


## ICA
```{r}
set.seed(101)

n_components <- 2
ica_result <- fastICA(wdbc, n.comp = n_components)

ica_data <- data.frame(ica_result$S)
colnames(ica_data) <- paste0("IC", 1:n_components)

ica_km.res <- kmeans(ica_data, 2, nstart = 100)

get_kmeans_plot(ica_km.res, ica_data, "ICA")
RRand(encoded_diagnosis, ica_km.res$cluster)
```
Visually ICA provides clusters that are not very well-separated but with no overlap. With an Adjusted Rand Index of 0.7058, it shows minor improvement in the performance of the clustering method.


## t-Distributed Stochastic Neighbour Embedding (t-SNE)
```{r}
set.seed(101)

tsne_result <- Rtsne(wdbc, dims = 3, perplexity = 30) # dimension reduction to only 3
tsne_data <- as.data.frame(tsne_result$Y)

tsne_km.res <- kmeans(tsne_data, 2, nstart = 100)

get_kmeans_plot(tsne_km.res, tsne_data, "t-SNE")
RRand(encoded_diagnosis, tsne_km.res$cluster)
```
Visually t-SNE provides clusters that are well-separated with no overlap. With an Adjusted Rand Index of 0.7731, it shows a drastic improvement in the performance of the clustering method.


## Uniform Manifold Approximation and Projection (UMAP)
```{r}
set.seed(101)

umap_result <- umap(wdbc)
umap_data <- as.data.frame(umap_result$layout)

umap_km.res <- kmeans(umap_data, 2, nstart = 100)

get_kmeans_plot(umap_km.res, umap_data, "UMAP")
RRand(encoded_diagnosis, umap_km.res$cluster)
```
Finally, visually UMAP provides clusters that are very well-separated with no overlap. With an Adjusted Rand Index of 0.7794, it shows the highest improvement in the performance of the clustering method.

Looking at all of these results we can confidently say that dimensionality reduction techniques can significantly enhance the clustering performance.
