---
title: "DANA 4840 Project - Research Question 1"
author: "Aryan Mukherjee, Maryam Gadimova, Patricia Tating, Roman Shrestha"
output: html_document
---

What is the role of dimensionality reduction techniques such as Principal Component Analysis (PCA), Independent Component Analysis (ICA), t-Distributed Stochastic Neighbor Embedding (t-SNE) or Uniform Manifold Approximation and Projection (UMAP) in enhancing clustering performance?

## Loading the libraries
```{r loading_libraries, message = FALSE}
library("ggplot2")
library("factoextra")
library("dendextend")
library("hopkins")
library("corrplot")
library("cluster")
library("patchwork")
library("clValid")
library("EMCluster")
library("fastICA")
library("Rtsne")
library("umap")
library(mclust)
```

## Loading the Dataset
```{r}
wdbc <- read.csv("./data/wdbc.csv", header = T, sep = ",")
head(wdbc)
```

## Checking the internal structure
```{r}
dim(wdbc)
str(wdbc)
```

## Pre-processing and Normalizing
```{r}
color_palette <- rainbow(10) #color palette
diagnosis <- wdbc$diagnosis 
encoded_diagnosis <- ifelse(diagnosis == "M", 1, 2) #making diagnoses numerical

wdbc_numerical <- wdbc[, -c(1, 2)] #remove id and diagnosis

wdbc_scaled <- data.frame(scale(wdbc_numerical)) #scale data
rownames(wdbc_scaled) <- wdbc$ID
wdbc <- wdbc_scaled
head(wdbc)
```

## Assessing Clustering Tendency

## Hopkins Statistic
```{r}
set.seed(69)

hopkins_wdbc <- hopkins(wdbc, m = ceiling(nrow(wdbc) / 10))
hopkins_wdbc
```
This indicates highly clusterable data structure

## K-Means Graph
```{r}
get_kmeans_plot <- function(km.res, data, name) {
  p <- fviz_cluster(
    km.res,
    data = data,
    palette = color_palette,
    ellipse.type = "convex",
    star.plot = TRUE,
    ellipse = TRUE,
    geom = "point",
    main = paste0(name, " K-Means Cluster Plot"),
    ggtheme = theme_minimal()
  )

  return(p)
}
```

## Base
```{r}
set.seed(101)

km.res <- kmeans(wdbc, 2, nstart = 100)

get_kmeans_plot(km.res, wdbc, "Base")
RRand(encoded_diagnosis, km.res$cluster)
```

 The clusters are reasonably well-separated with no overlap


## PCA
```{r}
set.seed(101)

pca_wdbc <- prcomp(wdbc)
pca_index <- which(cumsum(summary(pca_wdbc)$importance[2,]) >= 0.8)[1]

pca_data <- data.frame(pca_wdbc$x)
pca_data_no <- pca_data[, 1:pca_index]

pca_km.res <- kmeans(pca_data_no, 2, nstart = 100)

get_kmeans_plot(pca_km.res, pca_data_no, "PCA")
RRand(encoded_diagnosis, pca_km.res$cluster)
```

PCA provides clusters that are not very well-separated but with slight overlap

## ICA
```{r}
set.seed(101)

n_components <- 2
ica_result <- fastICA(wdbc, n.comp = n_components)

ica_data <- data.frame(ica_result$S)
colnames(ica_data) <- paste0("IC", 1:n_components)

ica_km.res <- kmeans(ica_data, 2, nstart = 100)

get_kmeans_plot(ica_km.res, ica_data, "ICA")
RRand(encoded_diagnosis, ica_km.res$cluster)
```
ICA provides clusters that are not very well-separated  with no overlap

## t-SNE
```{r}
set.seed(101)

tsne_result <- Rtsne(wdbc, dims = 3, perplexity = 30)
tsne_data <- as.data.frame(tsne_result$Y)

tsne_km.res <- kmeans(tsne_data, 2, nstart = 100)

get_kmeans_plot(tsne_km.res, tsne_data, "t-SNE")
RRand(encoded_diagnosis, tsne_km.res$cluster)
```
t-sne provides clusters that are  well-separated with no overlap
```{r}
set.seed(101)

umap_result <- umap(wdbc)
umap_data <- as.data.frame(umap_result$layout)

umap_km.res <- kmeans(umap_data, 2, nstart = 100)

get_kmeans_plot(umap_km.res, umap_data, "UMAP")
RRand(encoded_diagnosis, umap_km.res$cluster)
```

## LDA
```{r}
lda_model <- lda(diagnosis ~ ., data = wdbc)
lda_values <- predict(lda_model)

lda_data <- data.frame(lda_values$x)
lda_data$diagnosis <- diagnosis

ggplot(lda_data, aes(x = LD1, y = 0, color = diagnosis)) +
  geom_point(size = 3) +
  labs(
    title = "LDA: Iris Data Projected onto the First Linear Discriminant",
    x = "Linear Discriminant 1",
    y = ""
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank()
  )
```
LDA does not separate the clusters well enough as there is still overlap after LDA



## Meila's Variation Index

```{r}
# Meila's Variation Index for Base
diagnosis_numeric <- as.numeric(as.factor(diagnosis))
result <- cluster.stats(d = dist(wdbc_numerical), km.res$cluster, diagnosis_numeric)
vi_index_base <- result$vi
print(paste("Base - Meila's Variation Index:", vi_index_base))
```
The VI value of 0.5772 suggests a moderate level of agreement between the clusters produced by our k-means algorithm and the true diagnostic categories.
## Meila's Variation Index for PCA
```{r}

result <- cluster.stats(d = dist(pca_data_no), pca_km.res$cluster, diagnosis_numeric)
vi_index_pca <- result$vi
print(paste("PCA - Meila's Variation Index:", vi_index_pca))

```
## Meila's Variation Index for t-SNE
```{r}

result <- cluster.stats(d = dist(tsne_data), tsne_km.res$cluster, diagnosis_numeric)
vi_index_tsne <- result$vi
print(paste("t-SNE - Meila's Variation Index:", vi_index_tsne))
```
## Meila's Variation Index for UMAP
```{r}

result <- cluster.stats(d = dist(umap_data), umap_km.res$cluster, diagnosis_numeric)
vi_index_umap <- result$vi
print(paste("UMAP - Meila's Variation Index:", vi_index_umap))
```

